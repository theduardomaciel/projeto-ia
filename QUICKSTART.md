# üöÄ Guia R√°pido de Configura√ß√£o e Teste

## 1. Configura√ß√£o Inicial

### Instalar Depend√™ncias

```bash
# Criar ambiente virtual
python -m venv venv

# Ativar ambiente
# Windows:
.\venv\Scripts\activate
# Linux/macOS:
source venv/bin/activate

# Instalar pacotes
pip install -r requirements.txt
```

### Configurar API Keys

Escolha **pelo menos um** provedor de LLM:

#### Op√ß√£o A: Google Gemini (Recomendado - Gratuito)

1. Acesse: https://aistudio.google.com/app/apikey
2. Fa√ßa login com conta Google
3. Clique em "Get API Key" ‚Üí "Create API Key"
4. Copie a chave gerada

#### Op√ß√£o B: DeepSeek (Alternativa gratuita)

1. Acesse: https://platform.deepseek.com/
2. Crie uma conta
3. V√° em "API Keys" e gere uma nova chave

#### Op√ß√£o C: Grok/xAI

1. Acesse: https://x.ai/
2. Cadastre-se para acesso √† API
3. Gere uma API key

### Criar arquivo .env

```bash
# Copie o template
cp .env.example .env

# Edite o arquivo .env e adicione sua(s) chave(s)
```

Exemplo de `.env` configurado:

```env
# Google Gemini (prioridade - API gratuita)
GEMINI_API_KEY=AIzaSy...sua_chave_aqui

# Configura√ß√µes do sistema
DEFAULT_LLM_PROVIDER=gemini
MAX_TOKENS=1000
TEMPERATURE=0.7
```

---

## 2. Testar Integra√ß√£o com LLM

### Teste B√°sico (Verificar se funciona)

```bash
# Execute o script de exemplos
python src/llm/examples.py
```

**Sa√≠da esperada:**
```
üöÄ Exemplos de uso do m√≥dulo LLM
============================================================

Exemplo 1: Chamada b√°sica
============================================================

‚úÖ Resposta (gemini/gemini-1.5-flash):
Pair programming √© uma t√©cnica √°gil onde dois desenvolvedores trabalham 
juntos em um √∫nico computador...

Tokens usados: 87
Lat√™ncia: 1.23s
```

### Teste Individual por Provedor

```python
# Crie um arquivo test_llm.py
from dotenv import load_dotenv
load_dotenv()

from src.llm.client import LLMFactory

# Teste Gemini
try:
    llm = LLMFactory.create("gemini")
    response = llm.call("Diga ol√°!", max_tokens=50)
    print(f"‚úÖ Gemini funcionando: {response.content}")
except Exception as e:
    print(f"‚ùå Gemini falhou: {e}")

# Teste DeepSeek
try:
    llm = LLMFactory.create("deepseek")
    response = llm.call("Diga ol√°!", max_tokens=50)
    print(f"‚úÖ DeepSeek funcionando: {response.content}")
except Exception as e:
    print(f"‚ùå DeepSeek falhou: {e}")
```

---

## 3. Validar Templates de Prompt

```python
from src.llm.prompts import get_prompt_manager

pm = get_prompt_manager()

# Lista templates dispon√≠veis
print("Templates dispon√≠veis:")
for template in pm.list_templates():
    print(f"  - {template}")

# Testa um template
prompt = pm.get(
    "extra√ß√£o de soft skills",
    resume_text="Jo√£o √© comunicativo e trabalha bem em equipe."
)
print("\nPrompt formatado:")
print(prompt[:200] + "...")
```

---

## 4. Testar Logging de Intera√ß√µes

```python
from src.llm.client import get_default_llm
from src.llm.utils import get_llm_logger

llm = get_default_llm()
logger = get_llm_logger()

# Faz uma chamada
response = llm.call("Liste 3 linguagens de programa√ß√£o")

# Registra
logger.log_interaction(
    prompt="Liste 3 linguagens de programa√ß√£o",
    response=response.content,
    provider=response.provider,
    model=response.model,
    purpose="teste",
    tokens_used=response.tokens_used,
    latency=response.latency
)

# Ver estat√≠sticas
stats = logger.get_session_stats()
print(f"Total de chamadas: {stats['total_calls']}")
print(f"Tokens usados: {stats['total_tokens']}")

# Logs salvos em: logs/llm_session_YYYYMMDD_HHMMSS.jsonl
```

---

## 5. Compara√ß√£o entre Provedores (Documenta√ß√£o do Relat√≥rio)

Este teste √© importante para o relat√≥rio acad√™mico:

```python
from src.llm.client import LLMFactory
import time

prompt = "Explique o que √© CI/CD em 2 frases."

provedores = ["gemini", "deepseek", "grok"]
resultados = []

for provider in provedores:
    try:
        llm = LLMFactory.create(provider)
        
        start = time.time()
        response = llm.call(prompt, temperature=0.7, max_tokens=200)
        
        if response.success:
            resultados.append({
                "provider": provider,
                "resposta": response.content,
                "tokens": response.tokens_used,
                "latencia": response.latency,
                "qualidade": "‚úÖ"  # Avaliar manualmente
            })
    except Exception as e:
        print(f"‚ö†Ô∏è {provider} indispon√≠vel: {e}")

# Comparar resultados
for r in resultados:
    print(f"\n{'='*60}")
    print(f"Provedor: {r['provider'].upper()}")
    print(f"Resposta: {r['resposta']}")
    print(f"Tokens: {r['tokens']} | Lat√™ncia: {r['latencia']:.2f}s")
```

---

## 6. Troubleshooting

### Erro: "API key n√£o encontrada"

```bash
# Verifique se o arquivo .env existe
ls .env

# Verifique o conte√∫do (sem expor a chave completa)
cat .env | grep API_KEY

# Se estiver vazio, edite:
nano .env  # ou code .env
```

### Erro: "google-generativeai n√£o instalado"

```bash
# Reinstale as depend√™ncias
pip install -r requirements.txt

# Ou instale individualmente
pip install google-generativeai
```

### Erro: Rate limit exceeded

- **Solu√ß√£o**: Configure m√∫ltiplos provedores para fallback autom√°tico
- Gemini gratuito: ~15 requests/minuto
- Adicione delays entre chamadas se necess√°rio

### Erro: JSON inv√°lido na resposta

- **Causa**: LLM retornou texto fora do formato JSON esperado
- **Solu√ß√£o**: Use temperatura mais baixa (0.2-0.3) para respostas estruturadas
- O parser j√° tenta extrair JSON de texto com markdown

---

## 7. Pr√≥ximos Passos no Desenvolvimento

Ap√≥s validar o m√≥dulo LLM, continue com:

1. **M√≥dulo de Parsing** (`src/parsing/`)
   - Leitura de curr√≠culos `.txt`
   - Extra√ß√£o de blocos (experi√™ncia, forma√ß√£o, etc)

2. **SkillExtractor** (`src/skills/`)
   - Extra√ß√£o h√≠brida: regex + LLM
   - Valida√ß√£o de hard skills
   - An√°lise de soft skills

3. **ScoringEngine** (`src/scoring/`)
   - C√°lculo de pontua√ß√£o com pesos
   - Ranking de candidatos

4. **ExplainabilityEngine** (`src/explainability/`)
   - Gera√ß√£o de justificativas via LLM

5. **CLI** (`src/main.py`)
   - Interface completa
   - Orquestra√ß√£o de todo o pipeline

---

## 8. Checklist de Valida√ß√£o

- [ ] Ambiente virtual criado e ativado
- [ ] Depend√™ncias instaladas (`pip list` mostra `google-generativeai`)
- [ ] Arquivo `.env` criado com pelo menos uma API key
- [ ] Teste b√°sico executado com sucesso (`python src/llm/examples.py`)
- [ ] Pelo menos um provedor funcionando
- [ ] Templates de prompt carregando corretamente
- [ ] Logs sendo salvos em `logs/`
- [ ] Compara√ß√£o entre provedores testada (para relat√≥rio)

---

## 9. Documenta√ß√£o para o Relat√≥rio

**Registre estas informa√ß√µes:**

### Intera√ß√£o 1: Configura√ß√£o inicial do m√≥dulo LLM
- **Pergunta**: Como integrar m√∫ltiplos provedores LLM mantendo c√≥digo desacoplado?
- **Resposta do LLM**: Sugest√£o de usar padr√£o Abstract Factory com interface `LLMClient`
- **Decis√£o**: Adotada - permite trocar provedores sem reescrever c√≥digo
- **Resultado**: Implementa√ß√£o bem-sucedida com Gemini, DeepSeek, Grok

### Intera√ß√£o 2: Tratamento de respostas JSON
- **Problema**: LLMs √†s vezes retornam JSON dentro de markdown (```json)
- **Solu√ß√£o do LLM**: Parser robusto que tenta m√∫ltiplas estrat√©gias
- **Implementa√ß√£o**: M√©todo `_parse_json_response()` com fallbacks
- **Efic√°cia**: ‚úÖ Alta - funciona com diferentes formatos de resposta

### Compara√ß√£o de Provedores (preencher ap√≥s testes)

| Provedor | Lat√™ncia M√©dia | Qualidade das Respostas | Rate Limit | Custo |
|----------|---------------|------------------------|------------|-------|
| Gemini   | X.XXs        | [avaliar]              | ~15/min    | Gr√°tis|
| DeepSeek | X.XXs        | [avaliar]              | [testar]   | Gr√°tis|
| Grok     | X.XXs        | [avaliar]              | [testar]   | ?     |

---

## Suporte

Em caso de d√∫vidas ou problemas:
1. Verifique os logs em `logs/llm_session_*.jsonl`
2. Consulte a documenta√ß√£o em `docs/ARCHITECTURE.md`
3. Execute os exemplos em `src/llm/examples.py` para debugging
