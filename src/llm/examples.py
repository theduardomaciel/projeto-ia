"""
Exemplo de uso do m√≥dulo LLM
Demonstra como utilizar os diferentes clientes
"""

import sys
import os

# Adiciona o diret√≥rio raiz ao path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from llm.client import LLMFactory, get_default_llm, LLMResponse
from llm.prompts import get_prompt_manager, get_prompt
from llm.utils import get_llm_logger, retry_on_failure


def example_basic_call():
    """Exemplo 1: Chamada b√°sica ao LLM"""
    print("=" * 60)
    print("Exemplo 1: Chamada b√°sica")
    print("=" * 60)

    # Cria cliente Gemini
    try:
        llm = LLMFactory.create("gemini")

        prompt = "Explique em 2 frases o que √© pair programming."
        response = llm.call(prompt, temperature=0.7, max_tokens=200)

        if response.success:
            print(f"\n‚úÖ Resposta ({response.provider}/{response.model}):")
            print(response.content)
            print(f"\nTokens usados: {response.tokens_used}")
            print(f"Lat√™ncia: {response.latency:.2f}s")
        else:
            print(f"‚ùå Erro: {response.error}")

    except Exception as e:
        print(f"‚ùå Erro ao criar cliente: {e}")


def example_json_response():
    """Exemplo 2: Resposta em JSON estruturado"""
    print("\n" + "=" * 60)
    print("Exemplo 2: Resposta JSON estruturada")
    print("=" * 60)

    try:
        llm = get_default_llm()

        prompt = """
Extraia as seguintes informa√ß√µes do texto:

Texto: "Jo√£o Silva √© desenvolvedor Python h√° 5 anos. Tem experi√™ncia com Django e FastAPI."

Retorne em JSON com esta estrutura:
{
    "name": "nome da pessoa",
    "years_experience": n√∫mero de anos,
    "technologies": ["lista", "de", "tecnologias"]
}
"""

        result = llm.call_json(prompt, temperature=0.3, max_tokens=300)

        print("\n‚úÖ JSON parseado:")
        import json

        print(json.dumps(result, indent=2, ensure_ascii=False))

    except Exception as e:
        print(f"‚ùå Erro: {e}")


def example_with_template():
    """Exemplo 3: Uso de templates de prompt"""
    print("\n" + "=" * 60)
    print("Exemplo 3: Uso de prompt templates")
    print("=" * 60)

    try:
        llm = get_default_llm()
        pm = get_prompt_manager()

        # Lista templates dispon√≠veis
        print("\nTemplates dispon√≠veis:")
        for template in pm.list_templates():
            print(f"  - {template}")

        # Usa template de extra√ß√£o de contato
        resume_text = """
Jo√£o Silva
Email: joao@example.com
Tel: (11) 98765-4321
GitHub: github.com/joaosilva
"""

        prompt = get_prompt(
            "extra√ß√£o de informa√ß√µes de contato", resume_text=resume_text
        )

        result = llm.call_json(prompt, temperature=0.2)

        print("\n‚úÖ Informa√ß√µes extra√≠das:")
        import json

        print(json.dumps(result, indent=2, ensure_ascii=False))

    except Exception as e:
        print(f"‚ùå Erro: {e}")


def example_with_logging():
    """Exemplo 4: Logging de intera√ß√µes"""
    print("\n" + "=" * 60)
    print("Exemplo 4: Logging de intera√ß√µes")
    print("=" * 60)

    try:
        llm = get_default_llm()
        logger = get_llm_logger()

        prompt = "Liste 3 soft skills importantes para desenvolvedores."
        response = llm.call(prompt, temperature=0.7, max_tokens=200)

        # Registra intera√ß√£o
        logger.log_interaction(
            prompt=prompt,
            response=response.content,
            provider=response.provider,
            model=response.model,
            purpose="teste de logging",
            tokens_used=response.tokens_used,
            latency=response.latency,
            success=response.success,
            error=response.error,
        )

        if response.success:
            print(f"\n‚úÖ Resposta:")
            print(response.content)

        # Mostra estat√≠sticas
        stats = logger.get_session_stats()
        print(f"\nüìä Estat√≠sticas da sess√£o:")
        print(f"  Total de chamadas: {stats['total_calls']}")
        print(f"  Bem-sucedidas: {stats['successful_calls']}")
        print(f"  Tokens totais: {stats['total_tokens']}")
        print(f"  Lat√™ncia m√©dia: {stats['avg_latency']:.2f}s")
        print(f"\nüìù Logs salvos em: {logger.session_file}")

    except Exception as e:
        print(f"‚ùå Erro: {e}")


@retry_on_failure(max_retries=3, delay=1.0)
def example_with_retry():
    """Exemplo 5: Retry autom√°tico"""
    print("\n" + "=" * 60)
    print("Exemplo 5: Retry autom√°tico em falhas")
    print("=" * 60)

    llm = get_default_llm()

    prompt = "Diga ol√° em 3 idiomas diferentes."
    response = llm.call(prompt, max_tokens=100)

    if response.success:
        print(f"\n‚úÖ Resposta:")
        print(response.content)
    else:
        raise RuntimeError(f"Falha ap√≥s retries: {response.error}")


def example_compare_providers():
    """Exemplo 6: Compara√ß√£o entre provedores"""
    print("\n" + "=" * 60)
    print("Exemplo 6: Compara√ß√£o entre provedores")
    print("=" * 60)

    prompt = "Explique em uma frase o que √© CI/CD."

    providers = ["gemini", "groq", "openrouter"]

    for provider in providers:
        try:
            print(f"\nüîç Testando {provider.upper()}...")
            llm = LLMFactory.create(provider)
            response = llm.call(prompt, temperature=0.7, max_tokens=100)

            if response.success:
                print(f"‚úÖ {provider}: {response.content[:100]}...")
                print(
                    f"   Lat√™ncia: {response.latency:.2f}s | Tokens: {response.tokens_used}"
                )
            else:
                print(f"‚ùå {provider}: {response.error}")

        except Exception as e:
            print(f"‚ö†Ô∏è {provider} n√£o dispon√≠vel: {e}")


def main():
    """Executa todos os exemplos"""
    print("\nüöÄ Exemplos de uso do m√≥dulo LLM")
    print("=" * 60)

    # Carrega vari√°veis de ambiente
    from dotenv import load_dotenv

    load_dotenv()

    # Verifica se h√° API keys configuradas
    if (
        not os.getenv("GEMINI_API_KEY")
        and not os.getenv("GROQ_API_KEY")
        and not os.getenv("OPENROUTER_API_KEY")
    ):
        print("\n‚ö†Ô∏è AVISO: Nenhuma API key configurada!")
        print("Configure pelo menos uma das seguintes vari√°veis de ambiente:")
        print("  - GEMINI_API_KEY (https://aistudio.google.com/app/apikey)")
        print("  - GROQ_API_KEY (https://console.groq.com/keys)")
        print("  - OPENROUTER_API_KEY (https://openrouter.ai/keys)")
        print("\nOu crie um arquivo .env na raiz do projeto.")
        return

    try:
        example_basic_call()
        example_json_response()
        example_with_template()
        example_with_logging()
        example_with_retry()
        example_compare_providers()

    except KeyboardInterrupt:
        print("\n\n‚ùå Execu√ß√£o interrompida pelo usu√°rio")


if __name__ == "__main__":
    main()
